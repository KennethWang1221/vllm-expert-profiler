diff --git a/vllm/model_executor/layers/fused_moe/layer.py b/vllm/model_executor/layers/fused_moe/layer.py
index e68d35b31..40a4555e1 100644
--- a/vllm/model_executor/layers/fused_moe/layer.py
+++ b/vllm/model_executor/layers/fused_moe/layer.py
@@ -7,6 +7,9 @@ from collections.abc import Callable, Iterable
 from contextlib import nullcontext
 from enum import Enum
 from typing import Literal, cast, get_args, overload
+
+import json
+import os
 
 import torch
 import torch.nn.functional as F
@@ -68,6 +71,77 @@ from vllm.v1.worker.ubatching import dbo_current_ubatch_id
 
 logger = init_logger(__name__)
 
+_MOE_LOG_HEADER_WRITTEN = False
+
+
+def _should_log_moe_layer(layer_id: int) -> bool:
+    target = os.environ.get("VLLM_LOG_MOE_LAYER", "0")
+    try:
+        return layer_id == int(target)
+    except Exception:
+        return False
+
+
+def _write_moe_log_header(log_path: str, layer_id: int, top_k: int) -> None:
+    global _MOE_LOG_HEADER_WRITTEN
+    if _MOE_LOG_HEADER_WRITTEN:
+        return
+    if os.path.exists(log_path) and os.path.getsize(log_path) > 0:
+        _MOE_LOG_HEADER_WRITTEN = True
+        return
+    try:
+        import vllm
+
+        vllm_version = getattr(vllm, "__version__", "unknown")
+    except Exception:
+        vllm_version = "unknown"
+
+    seed = int(os.environ.get("VLLM_LOG_MOE_SEED", "0"))
+    model_id = os.environ.get("VLLM_LOG_MOE_MODEL_ID", "unknown")
+    device = "cpu"
+    if torch.cuda.is_available():
+        try:
+            device = torch.cuda.get_device_name(0)
+        except Exception:
+            device = "cuda"
+
+    meta = {
+        "type": "meta",
+        "model_id": model_id,
+        "vllm_version": vllm_version,
+        "torch_version": torch.__version__,
+        "device": device,
+        "seed": seed,
+        "layers_logged": [layer_id],
+        "top_k": top_k,
+    }
+    with open(log_path, "a") as f:
+        f.write(json.dumps(meta) + "\n")
+    _MOE_LOG_HEADER_WRITTEN = True
+
+
+def _log_moe_routes(
+    log_path: str,
+    layer_id: int,
+    topk_ids: torch.Tensor,
+    topk_weights: torch.Tensor,
+    token_offset: int = 0,
+) -> None:
+    if not _should_log_moe_layer(layer_id):
+        return
+    _write_moe_log_header(log_path, layer_id, int(topk_ids.size(1)))
+    req_id = os.environ.get("VLLM_LOG_MOE_REQ_ID", "r1")
+    ids = topk_ids.detach().cpu().tolist()
+    weights = topk_weights.detach().cpu().tolist()
+    with open(log_path, "a") as f:
+        for token_idx, (tok_ids, tok_weights) in enumerate(zip(ids, weights)):
+            record = {
+                "type": "route",
+                "req_id": req_id,
+                "token_idx": token_offset + token_idx,
+                "layer": layer_id,
+                "topk_ids": tok_ids,
+                "topk_weights": tok_weights,
+            }
+            f.write(json.dumps(record) + "\n")
+
 
 class FusedMoeWeightScaleSupported(Enum):
     TENSOR = "tensor"
@@ -1687,6 +1761,15 @@ class FusedMoE(nn.Module, CustomOp):
                     hidden_states=staged_hidden_states,
                     router_logits=staged_router_logits,
                 )
+                log_path = os.environ.get("VLLM_LOG_MOE")
+                if log_path:
+                    _log_moe_routes(
+                        log_path=log_path,
+                        layer_id=self.layer_id,
+                        topk_ids=topk_ids,
+                        topk_weights=topk_weights,
+                        token_offset=chunk_start,
+                    )
 
                 if self.capture is not None:
                     self.capture(topk_ids)
@@ -1880,6 +1963,15 @@ class FusedMoE(nn.Module, CustomOp):
                     hidden_states=x_orig,
                     router_logits=router_logits,
                 )
+                log_path = os.environ.get("VLLM_LOG_MOE")
+                if log_path:
+                    _log_moe_routes(
+                        log_path=log_path,
+                        layer_id=self.layer_id,
+                        topk_ids=topk_ids,
+                        topk_weights=topk_weights,
+                        token_offset=0,
+                    )
 
                 if self.capture is not None:
                     self.capture(topk_ids)
